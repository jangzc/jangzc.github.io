---
layout:     post
title:      K-近邻算法(KNN)之案例一
subtitle:   《机器学习实战二》
date:       2018-08-07
author:     Jang
header-img: img/post-bg-debug.png
catalog: true
tags:
    - MachineLearning
    - 机器学习实战
---

# 1. 项目案例：优化约会网站的配对效果

# 2. 项目概述
海伦使用约会网站寻找约会对象。经过一段时间之后，她发现曾交往过三种类型的人:

* 不喜欢的人
* 魅力一般的人
* 极具魅力的人

她希望：

* 工作日与魅力一般的人约会
* 周末与极具魅力的人约会
* 不喜欢的人则直接排除掉

现在她收集到了一些约会网站未曾记录的数据信息，这更有助于匹配对象的归类。

# 3. 开发流程
```
收集数据: 提供文本文件
准备数据：使用Python解析文本文件
分析数据：使用Matplotlib画二维散点图
训练算法：此步骤不适用于k近邻算法
测试算法：使用部分数据作为测试样本
使用算法：产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型
```
## 3.1 收集数据<br>
[数据文本文件](https://github.com/jangzc/MachineLearningPractise/blob/master/KNN_P1/dateSet.txt),共有1000行.<br>
海伦约会的对象主要包括以下三种特征:<br>
* 每年获得的飞行常客里程数
* 玩视频游戏所耗时间百分比
* 每周消费的冰淇淋公升数

文本文件数据格式如下:
```
40920	8.326976	0.953952	3
14488	7.153469	1.673904	2
26052	1.441871	0.805124	1
75136	13.147394	0.428964	1
38344	1.669788	0.134296	1
```

## 3.2 准备数据<br>
将文本记录转换为 NumPy的解析程序:
```
def file2matrix(filename)
    '''
    Desc:
        导入训练数据
    paramaters:
        filename: 数据文件路径
    return:
        数据矩阵 returnMat 和对应的类别 classLabelVector
    '''
    fr = open(filename)
    # 获得文件中的数据行的行数
    numberOfLines = len(fr.readlines())
    # 生成对应的空矩阵
    returnMat = zeros((numberOfLines,3))
    classLabelVector = []
    
    fr = open(filename)
    index = 0
    for line in fr.readlines():
        line = line.strip()
        # 以'\t'切割字符串
        listFromLine = line.split('\t')
        # 每列的属性数据
        returnMat[index,:] = listFromLine[0:3]
        # 每列的类别数据
        classLabelVector.append(int(listFromLine[-1]))
        index += 1
    # 返回数据矩阵returnMat和对应的类别classLabelVector
    return returnMat, classLabelVector
```

## 3.3 分析数据<br>
* 使用Matplotlib画二维散点图
```
import matplotlib
import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_subplot(111)
ax.scatter(datingDataMat[:,0],datingDataMat[:,1],15.0*array(datingLabels),15.0*array(datingLabels))
plt.show()
```
<img src="https://github.com/apachecn/MachineLearning/raw/master/images/2.KNN/knn_matplotlib_2.png"/>


* 归一化数据<br>
归一化特征值，消除特征之间量级不同导致的影响<br>
归一化定义：归一化就是要把你需要处理的数据，经过处理后(通过某种算法)限制在你需要的一定范围之内。首先归一化就为了后面数据处理的方便，其次是保证程序运行时收敛加快。方法有如下：
    * 1). 线性函数转换，表达式如下:<br>
        y=(x-MinValue)/(MaxValue-MinValue)
    * 2). 对数函数转换，表达式如下:<br>
        y=log10(x)
    * 3). 反余切函数转换, 表达式如下:<br>
        y=arctan(x) * 2/PI

在统计学中，归一化的具体作用是归纳同一样本的统计分布性。归一化在0-1之间是统计的概率分布，归一化在-1-+1之间是统计的坐标分布
```
def autoNorm(dataSet):
    '''
    Desc:
        归一化特征值, 消除特征值之间量级不同导致的影响
    parameters:
        dataSet:数据集
    return:
        归一化后的数据集
    归一化公式：
        Y = (X-Xmin)/(Xmax-Xmin)
    '''
    # 计算每种属性的最大值、最小值、范围
    minVals = dataSet.min(0)
    maxVals = dataSet.max(0)
    # 极差
    ranges = maxVals - minVals
    normDataSet = zeros(shape(dataSet))
    m = dataSet.shape[0]
    # 生成与最小值之差组成的矩阵
    normDataSet - dataSet - tile(minVals,(m,1))
    # 将最小值之差除以范围组成的矩阵
    normDataSet = normDataSet/ tile(ranges,(m,1))
    return normDataSet, ranges, minVals
```

## 3.4 训练算法: 此步骤不适用与k-近邻算法<br>
因为测试数据每一次都要与全部的训练数据进行比较，所以这个过程是没有必要的

kNN算法伪代码:
```
对于每一个在数据集中的数据点：
    计算目标的数据点(需要分类的数据点)与该数据点的距离
    将距离排序:从小到大
    选取前K个最短距离
    选择这K个中最多的分类类别
    返回该类别来作为目标数据点的预测值
```

```
def classify0(inX,dataSet,labels,k):
    dataSetSize = dataSet.shape[0]
    #距离度量，度量公式为欧式距离
    
```
