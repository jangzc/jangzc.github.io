---
layout:     post
title:      K-近邻算法(KNN)之案例一
subtitle:   《机器学习实战二》
date:       2018-08-07
author:     Jang
header-img: img/post-bg-debug.png
catalog: true
tags:
    - MachineLearning
    - 机器学习实战
---

# 1. 项目案例：优化约会网站的配对效果

# 2. 项目概述
海伦使用约会网站寻找约会对象。经过一段时间之后，她发现曾交往过三种类型的人:

* 不喜欢的人
* 魅力一般的人
* 极具魅力的人

她希望：

* 工作日与魅力一般的人约会
* 周末与极具魅力的人约会
* 不喜欢的人则直接排除掉

现在她收集到了一些约会网站未曾记录的数据信息，这更有助于匹配对象的归类。

# 3. 开发流程
```
收集数据: 提供文本文件
准备数据：使用Python解析文本文件
分析数据：使用Matplotlib画二维散点图
训练算法：此步骤不适用于k近邻算法
测试算法：使用部分数据作为测试样本
使用算法：产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型
```
## 3.1 收集数据<br>
[数据文本文件](https://github.com/jangzc/MachineLearningPractise/blob/master/KNN_P1/dateSet.txt),共有1000行.<br>
海伦约会的对象主要包括以下三种特征:<br>
* 每年获得的飞行常客里程数
* 玩视频游戏所耗时间百分比
* 每周消费的冰淇淋公升数

文本文件数据格式如下:
```
40920	8.326976	0.953952	3
14488	7.153469	1.673904	2
26052	1.441871	0.805124	1
75136	13.147394	0.428964	1
38344	1.669788	0.134296	1
```

## 3.2 准备数据<br>
将文本记录转换为 NumPy的解析程序:
```
def file2matrix(filename)
    '''
    Desc:
        导入训练数据
    paramaters:
        filename: 数据文件路径
    return:
        数据矩阵 returnMat 和对应的类别 classLabelVector
    '''
    fr = open(filename)
    # 获得文件中的数据行的行数
    numberOfLines = len(fr.readlines())
    # 生成对应的空矩阵
    returnMat = zeros((numberOfLines,3))
    classLabelVector = []
    
    fr = open(filename)
    index = 0
    for line in fr.readlines():
        line = line.strip()
        # 以'\t'切割字符串
        listFromLine = line.split('\t')
        # 每列的属性数据
        returnMat[index,:] = listFromLine[0:3]
        # 每列的类别数据
        classLabelVector.append(int(listFromLine[-1]))
        index += 1
    # 返回数据矩阵returnMat和对应的类别classLabelVector
    return returnMat, classLabelVector
```

## 3.3 分析数据<br>
* 使用Matplotlib画二维散点图
```
import matplotlib
import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_subplot(111)
ax.scatter(datingDataMat[:,0],datingDataMat[:,1],15.0*array(datingLabels),15.0*array(datingLabels))
plt.show()
```
<img src="https://github.com/apachecn/MachineLearning/raw/master/images/2.KNN/knn_matplotlib_2.png"/>


* 归一化数据<br>
归一化特征值，消除特征之间量级不同导致的影响<br>
归一化定义：归一化就是要把你需要处理的数据，经过处理后(通过某种算法)限制在你需要的一定范围之内。首先归一化就为了后面数据处理的方便，其次是保证程序运行时收敛加快。方法有如下：
    * 1). 线性函数转换，表达式如下:<br>
        y=(x-MinValue)/(MaxValue-MinValue)
    * 2). 对数函数转换，表达式如下:<br>
        y=log10(x)
    * 3). 反余切函数转换, 表达式如下:<br>
        y=arctan(x) * 2/PI

在统计学中，归一化的具体作用是归纳同一样本的统计分布性。归一化在0-1之间是统计的概率分布，归一化在-1-+1之间是统计的坐标分布
```
def autoNorm(dataSet):
    '''
    Desc:
        归一化特征值, 消除特征值之间量级不同导致的影响
    parameters:
        dataSet:数据集
    return:
        归一化后的数据集
    归一化公式：
        Y = (X-Xmin)/(Xmax-Xmin)
    '''
    # 计算每种属性的最大值、最小值、范围
    minVals = dataSet.min(0)
    maxVals = dataSet.max(0)
    # 极差
    ranges = maxVals - minVals
    normDataSet = zeros(shape(dataSet))
    m = dataSet.shape[0]
    # 生成与最小值之差组成的矩阵
    normDataSet - dataSet - tile(minVals,(m,1))
    # 将最小值之差除以范围组成的矩阵
    normDataSet = normDataSet/ tile(ranges,(m,1))
    return normDataSet, ranges, minVals
```

## 3.4 训练算法: 此步骤不适用与k-近邻算法<br>
因为测试数据每一次都要与全部的训练数据进行比较，所以这个过程是没有必要的

kNN算法伪代码:
```
对于每一个在数据集中的数据点：
    计算目标的数据点(需要分类的数据点)与该数据点的距离
    将距离排序:从小到大
    选取前K个最短距离
    选择这K个中最多的分类类别
    返回该类别来作为目标数据点的预测值
```

```
def classify0(inX,dataSet,labels,k):
    dataSetSize = dataSet.shape[0]
    #距离度量，度量公式为欧式距离
    diffMat = tile(inX, (dataSetSize,1)) - dataSet
    sqDiffMat = diffMat**2
    sqDistances = sqDiffMat.sum(axis=1)
    distances = sqDistances**0.5
    
    #将巨鹿排序，从小到大
    sortedDisIndicies = distances.argsort()
    # 选取前K个最短距离，选取这K个中最多的分类类别
    classCount={}
    for i in range(k):
        vateIlabel = labels[sortedDisIndicies[i]]
        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1
    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)
    return sortedClassCount[0][0]
```

## 3.5 测试算法: 使用海伦提供的部分数据作为测试样本。如果预测分类与实际分类不同，则标记为一个错误<br>
kNN分类器针对约会网站的测试代码：
```
def datingClassTest():
    '''
    Desc:
        对约会网站的测试方法
    parameters：
        none
    return:
        错误数
    '''
    # 设置测试数据的一个比例(训练数据集比例= 1-hoRatio)
    hoRatio = 0.1 # 测试范围，一部分测试一部分作为样本
    # 从文件中加载数据
    datingDataMat, datingLabels = file2matrix('./dateSet.txt')
    # 归一化数据
    normMat, ranges, minVals = autoNorm(datingDataMat)
    # m 表示数据的行数，即矩阵的第一维
    m = normMat.shape[0]
    # 设置测试的样本数量， numTestVecs:m 表示训练样本的数量
    numTestVecs = int(m * hoRatio)
    print 'numTestVecs=', numTestVecs
    errorCount = 0.0
    for i in range(numTestVecs):
        # 对数据测试
        classifierResult = classify0(normMat[i,:], normMat[numTestVecs:m,:], datingLabels[numTestVecs:m],3)
        print "the classifier came back with %d, the real answer is : %d" % (classifierResult, datingLabels[i])
        if(classifierResult != datingLabels[i]):
            errorCount += 1.0
    print "the total error rate is: %f" % (errorCount/float(numTestVecs))
    print errorCount
```

## 3.6 使用算法: 产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型.<br>
约会网站预测函数:
```
def classifyPerson():
    resultList = ['not at all','in samll doses','in large doses']
    percentTats = float(raw_input("percentage of time spent playing video games ?"))
    ffMiles = float(raw_input("frequent filer miles earned per year?"))
    iceCream = float(raw_input("liters of ice cream consumed per year?"))
    datingDataMat, datingLabels = file2matrix('./dateSet.txt')
    normMat, ranges, minVals = autoNorm(datingDataMat)
    inArr = array([ffMiles, percentTats, iceCream])
    classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels, 3)
    print "You will probably like this person: ", resultList[classifierResult - 1]
```
实际运行效果如下:
```
>>> classifyPerson()
percentage of time spent playing video games?10
frequent flier miles earned per year?10000
liters of ice cream consumed per year?0.5
You will probably like this person: in small doses
```
