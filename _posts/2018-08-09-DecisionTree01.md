---
layout:     post
title:      决策树(DecisionTree)之概述
subtitle:   《机器学习实战四》
date:       2018-08-09
author:     Jang
header-img: img/post-bg-debug.png
catalog: true
tags:
    - MachineLearning
    - 机器学习实战
---

# 1. 决策树概述
```
决策树(DecisionTree)算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一<br>
决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。<br>
决策树学习通常包括3个步骤：特征学者、决策树的生成和决策树的修剪。
```

# 2. 决策树场景
一个叫做 "二十个问题" 的游戏，游戏的规则很简单：参与游戏的一方在脑海中想某个事物，其他参与者向他提问，只允许提 20 个问题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围，最后得到游戏的答案。

一个邮件分类系统，大致工作流程如下：
<img src="https://github.com/apachecn/MachineLearning/raw/master/images/3.DecisionTree/%E5%86%B3%E7%AD%96%E6%A0%91-%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg"/>

决策树的定义:
分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成.<br>
结点有两种类型:内部结点(internal node)和叶结点(leaf node). 内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)<br>

用决策树对需要测试的实例进行分类：从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类中。

# 3. 决策树原理
# 3.1 决策树须知概念
信息熵 & 信息增益<br>
* 熵(entropy): 熵指的是体系的混乱程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量
* 信息论(information theory)中的熵(香农熵)：是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。
* 信息增益(information gain)：在划分数据集前后信息发生的变化称为信息增益。

# 3.2 决策树工作原理
如何构造一个决策树?
```
def createBranch():
'''
此处运用了迭代的思想。可以搜索迭代 recursion，甚至是dynamic programing.
'''
    检测数据集中的所有数据的分类标签是否相同：
        If so return 类标签
        Else:
            寻找划分数据集的最好特征(划分之后信息熵最小，也就是信息增益最大的特征)
            划分数据集
            创建分支节点
                for 每个划分的子集
                    调用函数 createBranch（创建分支的函数）并增加返回结果到分支节点中
            return 分支节点
```

# 3.3 决策树开发流程
```
收集数据：可以使用任何方法
准备数据：树构造算法(这里使用的是ID3算法，只适用与标称型数据，这就是为什么数值型数据必须离散化。还有其他树构造算法，比例CART)
分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期
训练算法：构造树的数据结构
测试算法：使用训练好的树计算错误率
使用算法：此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。
```

# 3.4 决策树算法特点
```
优点：计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征
缺点：容易过拟合
适用数据类型：数值型和标称型
```

