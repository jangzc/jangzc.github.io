---
layout:     post
title:      Logistic回归
subtitle:   《机器学习实战十一》
date:       2018-08-27
author:     Jang
header-img: img/post-bg-debug.png
catalog: true
tags:
    - MachineLearning
    - 机器学习实战
---

## Logistic回归概述<br>
Logistic回归或者叫逻辑回归，虽然名字有回归，但是它是用来做分类的。其主要思想是：根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类。<br>

## 须知概念<br>

### Sigmoid函数<br>

#### 回归概念<br>
假设现在有一些数据点，我们用一条直线对这些点进行拟合(这条直线称为最佳拟合直线)，这个拟合的过程就叫做回归。进而可以得到对这些点的拟合直线方程，那么我们根据这个回归方程，怎么进行分类呢？<br>

#### 二值行输出分类函数<br>
我们想要的函数应该是：能接受所有的输入然后预测出类别。例如，在两个类的情况下，上述函数输出0或者1. 该函数称为**海维塞得阶跃函数(Heaviside step function)**, 或者直接成为**单位阶跃函数**。然而，单位阶跃函数的问题在于：该函数在跳跃点上从0瞬间跳跃到1，这个瞬间跳跃过程有时很难处理。幸好，另一个函数也有类似的性质(可以输出0或者1的性质)，且数学上更易处理，这就是sigmoid函数。计算公式如下:<br>
<img src="https://github.com/apachecn/AiLearning/raw/dev/img/ml/5.Logistic/LR_1.png"/><br>

下图给出了Sigmoid函数在不同坐标尺下的两条曲线图。当x为0时，Sigmoid函数值为0.5。随着x的增大，对应的Sigmoid值将逼近于1; 而随着x的减小，Sigmoid值将逼近于0。如果横坐标刻度足够大，Sigmoid函数看起来很像一个阶跃函数.<br>
<img src="https://github.com/apachecn/AiLearning/raw/dev/img/ml/5.Logistic/LR_3.png"/><br>

因此，为了实现Logistic回归分类器，我们可以在每个特征上都乘以一个回归系数(如下公式所示), 然后把所有结果值相加，将这个总和带入Sigmoid函数中，进而得到一个范围在0~1之间的数值。任何大于0.5的数据被分入1类，小于0.5即被归入0类。所以，Logistic回归也是一种概率估计，比如这里Sigmoid函数得出得值为0.5,可以理解为给定数据和参数，数据被分入1类的概率为0.5。<br>

### 基于最优化方法的回归系数确定<br>
Sigmoid函数的输入记为z, 由下面公式得到:<br>
**z=w0x0+w1x1+w2x2+...+wnxn**<br>

如果采用向量的写法，上述公式可以写成 z=w<sup>T</sup>x, 它表示将这两个数值向量对应元素相乘然后全部加起来即得到z值。其中的向量x是分类器的输入数据，向量w也就是我们要找到的最佳参数(系数), 从而使得分类器尽可能地精确。为了寻找该最佳参数，需要用到最优化理论的一些知识。我们这里使用的是----梯度上升法(Gradient Ascent)。<br>

### 梯度上升法<br>

#### 梯度的介绍<br>

需要一点向量方面的数学知识<br>
```
向量 = 值 + 方向
梯度 = 向量
梯度 = 梯度值 + 梯度方向
```

#### 梯度上升法的思想<br>

要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为 ▽ ，则函数 f(x, y) 的梯度由下式表示:<br>
<img src="https://github.com/apachecn/AiLearning/raw/dev/img/ml/5.Logistic/LR_5.png"/><br>

这个梯度意味着要沿x的方向移动$ x=\frac{\partial f(x,y)}{\partial x} $, 沿y的方向移动$ x=\frac{\partial f(x,y)}{\partial y} $，其中函数f(x,y)必须要在待计算的点上有定义并且可微。下图是一个具体的例子<br>
<img src="https://github.com/apachecn/AiLearning/raw/dev/img/ml/5.Logistic/LR_8.png"/><br>

上图展示的，梯度上升算法到达每个点后都会重新估计移动的方向。从P0开始，计算完该点的梯度，函数就根据梯度移动到下一点P1。在P1点，梯度再次被重新计算，并沿着新的梯度方向移动到P2。如此循环迭代，知道满足停止条件。迭代过程中，梯度算子总是保证我们能选取到最佳的移动方向。<br>

上图中的梯度上升算法沿梯度方向一步。可以看到，梯度算子总是指向函数值增长最快的方向。这里所说的是移动方向，而未提到移动量的大小。该量值称为步长，记作$\alpha$。用向量来表示的话，梯度上升算法的迭代公式如下：<br>
$$ w:=w + \alpha \nabla wf(w) $$

该公式将一直被迭代执行，直至达到某个停止条件位置，比例迭代次数达到某个指定值或者算法达到某个可以允许的误差范围。<br>

介绍一下几个相关的概念:<br>
```
例如：y = w0 + w1x1 + w2x2 + ... +wnxn
梯度: 参考上图的例子，二维图像，x方向代表第一个系数，也就是w1, y方向代表第二个系数也就是w2, 这样的向量就是梯度.
a:上面的梯度算法的迭代公式中的阿尔法，这个代表的是移动步长(step length)。移动步长会影响最终结果的拟合程度，最好的方法就是随着迭代次数更改移动步长。
步长通俗的理解，100米，如果我一步走10米，我需要走10步；如果一步走20米，我只需要走5步。这里的一步走多少米就是步长的意思。
▽f(w)：代表沿着梯度变化的方向
```

问: 有人会好奇为什么有些书籍上说的是梯度下降法(Gradient Decent)?<br>

答：其实这两个方法在此情况下本质上是相同的。关键在于代价函数(cost function)或者叫目标函数(objective function)。如果目标函数是损失函数，那就是最小化损失函数来求函数的最小值，就用梯度下降。如果目标函数是似然函数(Likelihood function)，就是要最大化似然函数来求函数的最大值，那就用梯度上升。在逻辑回归中，损失函数和似然函数无非就是互为正负关系。<br>
只需要在迭代公式中的加法变成减法。因此，对应的公式可以写成<br>
$$ w:=w - \alpha \nabla wf(w) $$

#### 局部最优现象(Local Optima)<br>
<img src="https://github.com/apachecn/AiLearning/raw/dev/img/ml/5.Logistic/LR_20.png"/><br>
上图表示参数 θ 与误差函数 J(θ) 的关系图 (这里的误差函数是损失函数，所以我们要最小化损失函数)，红色的部分是表示 J(θ) 有着比较高的取值，我们需要的是，能够让 J(θ) 的值尽量的低。也就是深蓝色的部分。θ0，θ1 表示 θ 向量的两个维度（此处的θ0，θ1是x0和x1的系数，也对应的是上文w0和w1）。<br>

可能梯度下降的最终点并非是全局最小点，可能是一个局部最小点，如我们上图中的右边的梯度下降曲线，描述的是最终到达一个局部最小点，这是我们重新选择了一个初始点得到的。<br>

看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点。<br>

## Logistic回归原理<br>

### Logistic回归 工作原理<br>
```
每个回归系数初始化为1
重复 R 次：
    计算整个数据集的梯度
    使用 步长 x 梯度 更新回归系数的向量
返回回归系数
```

### Logistic回归 开发流程
```
收集数据：采用任意方法收集数据
准备数据：由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳。
分析数据：采用任意方法对数据进行分析
训练算法：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数
测试算法：一旦训练步骤完成，分类将会很快
使用算法：首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别，在这之后，我们就可以在输出的类别上做一些其他分析工作。
```

### Logistic回归 算法特点
```
优点：计算代价不高，易于理解和实现。
缺点：容易欠拟合，分类精度可能不高。
适用数据类型：数值型和标称型数据。
```
