---
layout:     post
title:      Logistic回归
subtitle:   《机器学习实战十一》
date:       2018-08-27
author:     Jang
header-img: img/post-bg-debug.png
catalog: true
tags:
    - MachineLearning
    - 机器学习实战
---

## Logistic回归概述<br>
Logistic回归或者叫逻辑回归，虽然名字有回归，但是它是用来做分类的。其主要思想是：根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类。<br>

## 须知概念<br>

### Sigmoid函数<br>

#### 回归概念<br>
假设现在有一些数据点，我们用一条直线对这些点进行拟合(这条直线称为最佳拟合直线)，这个拟合的过程就叫做回归。进而可以得到对这些点的拟合直线方程，那么我们根据这个回归方程，怎么进行分类呢？<br>

#### 二值行输出分类函数<br>
我们想要的函数应该是：能接受所有的输入然后预测出类别。例如，在两个类的情况下，上述函数输出0或者1. 该函数称为**海维塞得阶跃函数(Heaviside step function)**, 或者直接成为**单位阶跃函数**。然而，单位阶跃函数的问题在于：该函数在跳跃点上从0瞬间跳跃到1，这个瞬间跳跃过程有时很难处理。幸好，另一个函数也有类似的性质(可以输出0或者1的性质)，且数学上更易处理，这就是sigmoid函数。计算公式如下:<br>
<img src="https://github.com/apachecn/AiLearning/raw/dev/img/ml/5.Logistic/LR_1.png"/><br>

下图给出了Sigmoid函数在不同坐标尺下的两条曲线图。当x为0时，Sigmoid函数值为0.5。随着x的增大，对应的Sigmoid值将逼近于1; 而随着x的减小，Sigmoid值将逼近于0。如果横坐标刻度足够大，Sigmoid函数看起来很像一个阶跃函数.<br>
<img src="https://github.com/apachecn/AiLearning/raw/dev/img/ml/5.Logistic/LR_3.png"/><br>

因此，为了实现Logistic回归分类器，我们可以在每个特征上都乘以一个回归系数(如下公式所示), 然后把所有结果值相加，将这个总和带入Sigmoid函数中，进而得到一个范围在0~1之间的数值。任何大于0.5的数据被分入1类，小于0.5即被归入0类。所以，Logistic回归也是一种概率估计，比如这里Sigmoid函数得出得值为0.5,可以理解为给定数据和参数，数据被分入1类的概率为0.5。<br>

### 基于最优化方法的回归系数确定<br>
Sigmoid函数的输入记为z, 由下面公式得到:<br>
**z=w0x0+w1x1+w2x2+...+wnxn**<br>

如果采用向量的写法，上述公式可以写成 z=w<sup>T</sup>x, 它表示将这两个数值向量对应元素相乘然后全部加起来即得到z值。其中的向量x是分类器的输入数据，向量w也就是我们要找到的最佳参数(系数), 从而使得分类器尽可能地精确。为了寻找该最佳参数，需要用到最优化理论的一些知识。我们这里使用的是----梯度上升法(Gradient Ascent)。<br>

### 梯度上升法<br>

#### 梯度的介绍<br>

需要一点向量方面的数学知识<br>
```
向量 = 值 + 方向
梯度 = 向量
梯度 = 梯度值 + 梯度方向
```

#### 梯度上升法的思想<br>

要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为 ▽ ，则函数 f(x, y) 的梯度由下式表示:<br>
<img src="https://github.com/apachecn/AiLearning/raw/dev/img/ml/5.Logistic/LR_5.png"/><br>

这个梯度意味着要沿x的方向移动

$$
x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}
$$

