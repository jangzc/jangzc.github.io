---
layout:     post
title:      朴素贝叶斯之概述
subtitle:   《机器学习实战七》
date:       2018-08-20
author:     Jang
header-img: img/post-bg-debug.png
catalog: true
tags:
    - MachineLearning
    - 机器学习实战
---

## 1. 朴素贝叶斯概述<br>
贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础--贝叶斯定理。然后通过实例来讨论贝叶斯分类中最简单的一种：朴素贝叶斯分类.

### 1.1 贝叶斯理论 & 条件概率
#### 1.1.1 贝叶斯理论
我们现在有一个数据集，它由两类数据组成，数据分布如下：
<img src="https://github.com/apachecn/AiLearning/blob/master/images/4.NaiveBayesian/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83.png"/>
我们现在用p1(x,y)表示数据点(x,y)属于类别1的概率，用p2(x,y)表示属于类别2的概率，那么对于一个新数据点(x,y)，可以用下面的规则来判断它的类别：
* 如果p1(x,y)>p2(x,y)，那么类别为1
* 如果p2(x,y)>p1(x,y)，那么类别为2
也就说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。

#### 1.1.2 条件概率
有一个装了7块石头的罐子，其中3块是白色的，4块是黑色的。如果从罐子中随机取出一块石头，那么是白色石头的可能性是多少？由于取石头有7中可能，其中3种为白色，所以取出白色石头的概率为3/7. 那么取到黑色的概率是4/7. 我们使用p(white)来表示取白色的概率，其概率值可以通过白色石头数除以总数来得到。
<img src="https://github.com/apachecn/AiLearning/blob/master/images/4.NaiveBayesian/NB_2.png"/>

如果这7块石头放在两个统中，那么概率应该如何计算<br>
<img src="https://github.com/apachecn/AiLearning/blob/master/images/4.NaiveBayesian/NB_3.png"/><br>

计算P(white)或者P(black)，如果事先我们知道石头所在的桶的信息是会改变结果的。这就是所谓的条件概率(conditional probablity)。假定计算的是从B桶取到白色石头的概率，这个概率可以记作P(white|bucketB)，我们称之为"在已知石头出自B桶的条件下，取出白色石头的概率"。很容易得到，P(white|bucketA)为2/4, P(white/bucketB)为1/3.

条件概率的计算公式如下：
P(white|bucketB)=P(white and bucketB)/P(bucketB)

首先,我们用B桶中白色石头的个数除以两个桶中总的石头数，得到P(white and bucketB) = 1/7，其次，由于B桶中有三块石头，而总石头数为7，于是P(bucketB)就等于3/7。于是有P(white|bucketB)=P(white and bucketB)/P(bucketB) = (1/7)/(3/7) = 1/3<br>

另外一宗有效计算条件概率的方法称为贝叶斯准则。贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已经P(x|c)，要求P(c|x)，那么可以用下面的方法计算：<br>
<img src="https://github.com/apachecn/AiLearning/blob/master/images/4.NaiveBayesian/NB_4.png"/>

### 1.2 使用条件概率来分类
上面我们提到贝叶斯决策理论要求计算两个概率p1(x,y)和p2(x,y):
* 如果p1(x,y)>p2(x,y)，那么类别为1
* 如果p2(x,y)>p1(x,y)，那么类别为2
这并不是贝叶斯决策理论的所有内容,真正需要计算和比较的是p(c1|x,y)和p(c2|x,y).这些符号代表的意义是:给个某个由x,y表示的数据点，那么该数据点来自类别c1的概率是多少？来自c2的概率又是多少？注意这些概率与概率p(x,y|c1)并不一样，不过可以使用贝叶斯准则来交换概率中条件与结果.<br>
<img src="https://github.com/apachecn/AiLearning/blob/master/images/4.NaiveBayesian/NB_5.png"/>

使用上面这些定义，可以定义贝叶斯分类准则为：<br>
* 如果P(c1|x,y) > P(c2|x,y)，那么属于类别c1;
* 如果P(c1|x,y) < P(c2|x,y)，那么属于类别c2;

在文档分类中，整个文档(如一封电子邮件)是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词数目一样多.

我们假设特征之间相互独立。所谓**独立**(independence)指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系，比如说，"我们"中的"我"和"们"出现的概率与这两个字相邻没有任何关系。这个假设正是朴素贝叶斯分类器中朴素(naive)一词的含义。朴素贝叶斯分类器的另一个假设是，**每个特征同等重要**<br>

Note: 朴素贝叶斯分类器通常有两种实现方式：一种基于伯努利模型实现，一种基于多项式模型实现。这里采用前一种实现方式。该实现方式中并不考虑词在文档中出现的次数，只考虑出不出现，因为在这个意义上相当于假设词是等权重的。

## 2. 朴素贝叶斯场景<br>
机器学习的一个重要应用就是文档的自动分类

在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多

朴素贝叶斯是上面介绍的贝叶斯分类其的一个扩展，**是用于文档分类的常用算法.**

## 3. 朴素贝叶斯原理

### 朴素贝叶斯工作原理
```
提取所有文档中的词条并进行驱虫
获取文档的所有类别
计算每个类别中的文档数目
对每篇训练文档:
    对每个类别:
        如果词条出现在文档中-->增加该词条的计数(for循坏或者矩阵相加)
        增加所有词条的计数值(此类别下词条总数)
对每个类别：
    对每个词条：
        将该词条的数目除以总词条数目得到的条件概率(P(词条|类别))
返回该文档属于每个类别的条件概率(P(类别|文档的所有词条))
```
### 朴素贝叶斯开发流程
```
收集数据:可以使用任何方法
准备数据:需要数值型或者布尔型数据
分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好
训练算法：计算不同的独立特征的条件概率
测试算法：计算错误率
使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本
```

### 朴素贝叶斯算法特点
```
优点：在数据较少的情况下仍然有效，可以处理多类别问题
缺点：对于输入数据的准备方式较为敏感
使用数据类型：标称型数据
```


